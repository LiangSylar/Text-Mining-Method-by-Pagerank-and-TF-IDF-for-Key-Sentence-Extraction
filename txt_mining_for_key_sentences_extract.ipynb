{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ERG3020 Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEKr3dZzBMGB",
        "outputId": "3f66fc9a-9ac4-45e6-b08e-b9493a402bfa"
      },
      "source": [
        "# Chenang Li 118010135\n",
        "# Jialu Liang 118010164\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGr3pmjOBO5I"
      },
      "source": [
        "file_name = ['1.txt','2.txt','3.txt','4.txt','5.txt']\n",
        "file = []\n",
        "for i in range(len(file_name)):\n",
        "  file.append([])\n",
        "  with open(file_name[i], encoding='utf-8') as file_obj:\n",
        "    contents = file_obj.read()\n",
        "    file[i]=(re.split('[.\\n]',contents.rstrip()))\n",
        "  for j in file[i]:\n",
        "    if (len(j)<=2):\n",
        "      file[i].remove(j)\n",
        "# POS (Parts Of Speech) for: nouns, adjectives, verbs and adverbs\n",
        "DI_POS_TYPES = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'} \n",
        "POS_TYPES = list(DI_POS_TYPES.keys())\n",
        "\n",
        "# Constraints on tokens\n",
        "MIN_STR_LEN = 3\n",
        "RE_VALID = '[a-zA-Z]'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0dJbpjDIvtq",
        "outputId": "fa5a99b7-33cc-441b-ab30-ce56f948a7c1"
      },
      "source": [
        "# Get stopwords, stemmer and lemmatizer\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# Remove accents function\n",
        "def remove_accents(data):\n",
        "  return ''.join(x for x in unicodedata.normalize('NFKD', data) if x in string.ascii_letters or x == \" \")\n",
        "\n",
        "# Process all quotes\n",
        "def extract_text(num_file):\n",
        "  li_tokens = []\n",
        "  li_token_lists = []\n",
        "  li_lem_strings = []\n",
        "  for i,text in enumerate(file[num_file]):\n",
        "    # Tokenize by sentence, then by lowercase word\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "\n",
        "    # Process all tokens per quote\n",
        "    li_tokens_quote = []\n",
        "    li_tokens_quote_lem = []\n",
        "    for token in tokens:\n",
        "      # Remove accents\n",
        "      t = remove_accents(token)\n",
        "\n",
        "      # Remove punctuation\n",
        "      t = str(t).translate(string.punctuation)\n",
        "      li_tokens_quote.append(t)\n",
        "      \n",
        "      # Add token that represents \"no lemmatization match\"\n",
        "      li_tokens_quote_lem.append(\"-\") # this token will be removed if a lemmatization match is found below\n",
        "\n",
        "      # Process each token\n",
        "      if t not in stopwords:\n",
        "        if re.search(RE_VALID, t):\n",
        "          if len(t) >= MIN_STR_LEN:\n",
        "            pos = nltk.pos_tag([t])[0][1][:2]\n",
        "            pos2 = 'n'  # set default to noun\n",
        "            if pos in DI_POS_TYPES:\n",
        "              pos2 = DI_POS_TYPES[pos]\n",
        "            \n",
        "            stem = stemmer.stem(t)\n",
        "            lem = lemmatizer.lemmatize(t, pos=pos2)  # lemmatize with the correct POS\n",
        "            \n",
        "            if pos in POS_TYPES:\n",
        "              li_tokens.append((t, stem, lem, pos))\n",
        "\n",
        "              # Remove the \"-\" token and append the lemmatization match\n",
        "              li_tokens_quote_lem = li_tokens_quote_lem[:-1] \n",
        "              li_tokens_quote_lem.append(lem)\n",
        "\n",
        "    # Build list of token lists from lemmatized tokens\n",
        "    li_token_lists.append(li_tokens_quote)\n",
        "    \n",
        "    # Build list of strings from lemmatized tokens\n",
        "    str_li_tokens_quote_lem = ' '.join(li_tokens_quote_lem)\n",
        "    li_lem_strings.append(str_li_tokens_quote_lem.split(\" \"))\n",
        "  return(li_lem_strings, li_token_lists)\n",
        "\n",
        "text_sentences=[]\n",
        "token_list=[]\n",
        "for i in range(len(file_name)):\n",
        "  text,token=extract_text(i)\n",
        "  text_sentences.append(text)\n",
        "  token_list.append(token)\n",
        "\n",
        "for sents in text_sentences[3]:\n",
        "  for wrds in sents:\n",
        "    print(wrds, end=' ')\n",
        "  print('')\n",
        "print(file[3][19])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "warren buffett - berkshire hathaway return - profitability - - first quarter - - year - reporting - nearly - - - loss - - first quarter - - due - - covid pandemic \n",
            "berkshire hathaway - brkb - - - sprawl conglomerate - own company range - - geico insurance giant - burlington northern santa - railroad - wellknown consumer brand - dairy queen - benjamin moore paint - duracell battery - post - net profit - - - \n",
            "- - - operating profit - - metric buffett prefers - highlight - - - - - \n",
            "berkshire hathaway say - - filing - - security - exchange commission saturday - revenue - many - - manufacturing - service - retailing business - experienced significant recovery - revenue - earnings - - - past - month - - economy - reopen \n",
            "- - - optimism - berkshire hathaway also continued - plow money - buying back - - stock - oppose - use cash - - - acquisition \n",
            "berkshire hathaway spent nearly - - \n",
            "- - - repurchase - - expensive class - share - well - - - affordable class - stock - - quarter \n",
            "even - - stock purchase - berkshire - massive cash pile - grown \n",
            "- - - - - - \n",
            "- - - cash - - balance sheet - - - - - \n",
            "- - - - end - - \n",
            "- berkshire - share - brka - - - - never split - stock - - - trading - - - \n",
            "- - share - - - split - order - - - affordable - average investor - close - - price - - - - - - friday \n",
            "berkshire - - class - stock - - rebound - - broader market - year \n",
            "- - - - nearly - - - far - - \n",
            "still - - invest expert wonder - buffett - lose - magic touch \n",
            "berkshire - - make bet - top tech stock - apple - aapl - - amazon - amzn - - - past - year - - lag - broader market - - late \n",
            "berkshire hathaway stock - - - - - - - past - year - - - - - - - - - double - - nasdaq - soar nearly - - \n",
            "buffett - - - berkshire vice chairman charlie munger - ajit jain - greg abel - - talk - - - stock market rebound - economy - well - - host - - issue later saturday - - company host - annual shareholder meeting \n",
            "- meeting - - - - held - los angeles instead - berkshire - hometown - omaha - nebraska - - - - virtual event - - second straight year due - covid - - - broadcast live - yahoo finance start - - - \n",
            " \n",
            "The meeting, which will be held in Los Angeles instead of Berkshire's hometown of Omaha, Nebraska, will be a virtual event for the second straight year due to Covid-19 and will be broadcast live on Yahoo Finance starting around 12:30 ET\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miYqz0hMBSVs"
      },
      "source": [
        "lib_wrds=[]\n",
        "num_sentences=0\n",
        "for txts in text_sentences:\n",
        "  num_sentences=num_sentences+len(txts)\n",
        "  for sents in txts:\n",
        "    for wrds in sents:\n",
        "      if(wrds!='-' and wrds!=''):\n",
        "        lib_wrds.append(wrds)\n",
        "lib_wrds=np.unique(lib_wrds)\n",
        "lib_wrds=sorted(lib_wrds)\n",
        "total_occur = {'-': 0, '': 0}\n",
        "for words in lib_wrds:\n",
        "  for txts in text_sentences:\n",
        "    for sents in txts:\n",
        "      if(words in sents):\n",
        "        if(words in total_occur.keys()):\n",
        "          total_occur[words] = total_occur[words]+1\n",
        "        else:\n",
        "          total_occur[words] = 1\n",
        "\n",
        "#print (sorted(total_occur.items(), key=lambda total_occur: total_occur[0]))\n",
        "idf = {'-': 0, '': 0}\n",
        "for kys in total_occur.keys():\n",
        "  idf[kys]=total_occur[kys]/(num_sentences+1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwLTlDNyWDVt"
      },
      "source": [
        "def cal_tf(num_file):\n",
        "  res=[]\n",
        "  for sents in text_sentences[num_file]:\n",
        "    tmp_tf=[0]*len(lib_wrds)\n",
        "    mx_cnt = {'-': 0, '': 0}\n",
        "    for words in sents:\n",
        "      if(words!='-' and words!=''):\n",
        "        for j in range(len(lib_wrds)):\n",
        "          if(words==lib_wrds[j]):\n",
        "            tmp_tf[j]=tmp_tf[j]+1\n",
        "            break;\n",
        "        if(words in mx_cnt.keys()):\n",
        "          mx_cnt[words]=mx_cnt[words]+1;\n",
        "        else: mx_cnt[words]=1;\n",
        "    mx=0\n",
        "    for kys in mx_cnt.keys():\n",
        "      mx=max(mx,mx_cnt[kys])\n",
        "    for j in range(len(lib_wrds)):\n",
        "      #tmp_tf[j]=tmp_tf[j]/len(sents)idf\n",
        "      if(mx!=0):\n",
        "        tmp_tf[j]=tmp_tf[j]/mx*idf[lib_wrds[j]]\n",
        "    res.append(tmp_tf)\n",
        "  return(res)\n",
        "      "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCSOOkiPN0dw",
        "outputId": "70fbcc4d-534d-418c-dc00-22bbd09f43f2"
      },
      "source": [
        "num_text=0 # Change this number to run algorithm on different articles\n",
        "crit=0.01\n",
        "top_k=3\n",
        "#num_div=3  # k^2/num_div edges will be selected\n",
        "vect=cal_tf(num_text)\n",
        "m_vect=np.matrix(vect)\n",
        "sent_map = np.zeros([len(vect),len(vect)])\n",
        "#cos_all=[]\n",
        "#non_zero_cos=0\n",
        "for i in range(len(vect)):\n",
        "  for j in range(len(vect)):\n",
        "    u1=np.array(m_vect[i])\n",
        "    u2=np.array(m_vect[j])\n",
        "    cs=np.sum(u1*u2)/(math.sqrt(np.sum(np.square(u1)))*math.sqrt(np.sum(np.square(u2))))\n",
        "    #print(cs,end=\" \")\n",
        "    if(i!=j):\n",
        "      if(abs(cs)>crit):\n",
        "        #non_zero_cos=non_zero_cos+1\n",
        "        #cos_all.append(cs)\n",
        "        sent_map[i][j]=1\n",
        "        sent_map[j][i]=1\n",
        "  #print('')\n",
        "# cos_all=np.array(cos_all)\n",
        "# boundry=int(min(non_zero_cos, m_vect.shape[0]*m_vect.shape[0]/num_div)) # How many edges \n",
        "# print(non_zero_cos > m_vect.shape[0]*m_vect.shape[0]/num_div)\n",
        "# min_cos_crt=sorted(cos_all[np.argpartition(cos_all,-boundry)[-boundry:]])[0]\n",
        "# #print(min_cos_crt)\n",
        "# for i in range(len(vect)):\n",
        "#   for j in range(len(vect)):\n",
        "#     if(sent_map[i][j]>=min_cos_crt):\n",
        "#       sent_map[i][j]=1\n",
        "#       sent_map[j][i]=1\n",
        "#     else:\n",
        "#       sent_map[i][j]=0\n",
        "#       sent_map[j][i]=0\n",
        "non_empty_nodes=np.apply_along_axis(sum, 0, sent_map)>0\n",
        "cnt_graph=np.matrix(sent_map)[non_empty_nodes]\n",
        "cnt_graph=cnt_graph[:,non_empty_nodes]\n",
        "#print(len(sent_map),len(file[num_text]))\n",
        "markov_graph=cnt_graph\n",
        "for i in range(cnt_graph.shape[0]):\n",
        "  markov_graph[:,i]=markov_graph[:,i]/np.sum(markov_graph[:,i])\n",
        "init=np.ones(cnt_graph.shape[0])/cnt_graph.shape[0]\n",
        "init=np.matrix(init).T\n",
        "#markov_graph.shape\n",
        "for i in range(100):\n",
        "  init=np.matmul(markov_graph,init)\n",
        "flg=-1\n",
        "res_importance=np.zeros(len(non_empty_nodes))\n",
        "for i in range(len(non_empty_nodes)):\n",
        "  if(non_empty_nodes[i]!=0):\n",
        "    flg=flg+1\n",
        "    res_importance[i]=init[flg]\n",
        "print(res_importance)\n",
        "# flg=-1\n",
        "# for sents in text_sentences[num_text]:\n",
        "#   flg=flg+1\n",
        "#   if(non_empty_nodes[flg]==0): continue\n",
        "#   for wrds in sents:\n",
        "#     print(wrds, end=' ')\n",
        "#   print('')\n",
        "top_imp=sorted(res_importance[np.argpartition(res_importance,-top_k)[-top_k:]],reverse=True)\n",
        "done=np.zeros(len(res_importance))\n",
        "for i in range(len(top_imp)):\n",
        "  for j in range(len(res_importance)):\n",
        "    if(res_importance[j]==top_imp[i] and done[j]==0):\n",
        "      done[j]=1\n",
        "      print(i+1,\":\",file[num_text][j])\n",
        "      break\n",
        "print(top_imp)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.06481481 0.0617284  0.0308642  0.03703704 0.0154321  0.01234568\n",
            " 0.06790123 0.04320988 0.0617284  0.03703704 0.0154321  0.04012346\n",
            " 0.0154321  0.0308642  0.04012346 0.01851852 0.         0.02777778\n",
            " 0.00925926 0.04012346 0.0308642  0.00925926 0.0462963  0.0462963\n",
            " 0.04012346 0.04012346 0.04012346 0.02777778 0.04938272]\n",
            "1 : \"Although it is counterfactual to assert that DAM (Diego Armando Maradona) wouldn't have died if he had been treated adequately, taking into account what was known about the days leading up to his death we agree that he would have had a better chance of survival if he had been treated in a healthcare facility according to medical best practice,\" reads the report\n",
            "2 : Diego Maradona was in agony for 12 hours and the medical team treating him was \"deficient, reckless and indifferent\" when faced with his possible death, according to a report from the medical board appointed to investigate his demise\n",
            "3 : The Argentine football great \"did not have full use of his mental faculties\" and could have had \"a better chance of survival\" if he had been admitted to a healthcare facility, the medical board concluded in its report, which will become of the part of the judicial investigation into this death, the prosecutor handling the case confirmed to CNN\n",
            "[0.0679012345679013, 0.06481481481481488, 0.06172839506172846]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}